# Gesture2Text

**Gesture2Text** — экспериментальная система распознавания жестов рук на основе методов машинного обучения, ориентированная на задачи инклюзивного человеко-компьютерного взаимодействия и ассистивных технологий. Проект исследует возможность организации базовой коммуникации с помощью заранее определённого набора жестов, распознаваемых стандартной веб-камерой, без использования речи, сенсорного ввода или специализированного оборудования.

Система предназначена для людей с нарушениями речи или моторных функций, а также для сценариев, в которых голосовое управление затруднено или нежелательно (шумная среда, требования приватности). Все вычисления выполняются локально, без использования облачных сервисов.

---

## Ключевые возможности

* Классификация жестов на основе классических методов машинного обучения
* Распознавание жестов в реальном времени с использованием веб-камеры
* Полностью офлайн-работа (без передачи данных в сеть)
* Лёгкий ML-пайплайн без нейросетевой инференции во время работы
* Использование **MediaPipe Hands** и **scikit-learn**
* Стабилизация предсказаний и фильтрация по уверенности
* Опциональный вывод результата через синтез речи
* Модульная архитектура, удобная для учебных и исследовательских задач

---

## Поддерживаемые жесты

Текущая версия системы поддерживает фиксированный набор целевых жестов:

* HELP
* STOP
* WATER
* PAIN
* YES
* NO
* CALL
* OK

Дополнительно в системе реализован специальный класс:

* **UNKNOWN** — используется для отнесения всех жестов, не входящих в обученный набор, и снижения числа ложных срабатываний

Класс `UNKNOWN` введён осознанно для повышения надёжности и безопасности системы. Он позволяет игнорировать неоднозначные или нераспознанные положения руки. Набор жестов может быть расширен в будущем путём сбора дополнительных данных и повторного обучения модели.

---

## Техническое описание

Модель обучается на самостоятельно собранном датасете и оценивается на отложенной выборке для проверки обобщающей способности по жестам и пользователям.

Обработка данных выполняется по следующему конвейеру:

### 1. Детекция и трекинг руки
Модуль MediaPipe Hands обнаруживает одну руку и возвращает 21 трёхмерную ключевую точку (landmark) для каждого кадра.

### 2. Извлечение признаков
Каждый кадр преобразуется в вектор признаков размерности 63:

* координаты центрируются относительно запястья
* масштаб руки нормализуется по длине среднего пальца
* координаты левой и правой руки приводятся к единой системе
* данные преобразуются в фиксированный одномерный вектор

### 3. Классификация
Используется пайплайн библиотеки scikit-learn:

* `StandardScaler` для нормализации признаков
* мультиномиальная логистическая регрессия для классификации

### 4. Постобработка

* временная стабилизация предсказаний
* пороги уверенности
* опциональный вывод результата через синтез речи

Выбранный подход обеспечивает интерпретируемость, вычислительную эффективность и возможность работы в реальном времени на потребительском оборудовании.

---

## Структура проекта

```
gesture2text/
├── src/
│   ├── app/
│   │   ├── main.py              # Entry point (optional wrapper)
│   │   └── run_camera.py        # Real-time webcam application
│   ├── ml/
│   │   ├── features.py          # Feature extraction logic
│   │   ├── train.py             # Model training script
│   │   └── predict_one.py       # Offline prediction / evaluation tool
│   ├── vision/
│   │   └── hand_tracker.py      # MediaPipe Hands wrapper
│   └── data/
│       ├── analyze_dataset.py   # Dataset inspection and sanity checks
│       ├── collector.py         # Interactive data collection tool
│       ├── raw/                 # Raw datasets (ignored by Git)
│       └── models/              # Trained models (ignored by Git)
├── requirements.txt
├── .gitignore
├── run.sh
├── run.command
└── run.bat
```

---

## Установка

### Требования

- Python версии 3.10 или выше
- Веб-камера
- macOS, Linux или Windows

### Установка

1. Клонировать репозиторий:

`git clone https://github.com/your-username/gesture2text.git
cd gesture2text`

2. Создать и активировать виртуальное окружение:


`python -m venv .venv`
`source .venv/bin/activate    # macOS / Linux`
`.venv\Scripts\activate       # Windows`

3. Установить зависимости:

`pip install -r requirements.txt`

---

## Запуск приложения

### macOS / Linux

`./run.sh`

или

`./run.command`

### Windows

`run.bat`

Также возможен прямой запуск:

`PYTHONPATH=src python src/app/run_camera.py`

После запуска открывается окно с изображением с камеры и результатами распознавания жестов в реальном времени.

---

## Data Collection

Для сбора новых примеров жестов:

`PYTHONPATH=src python src/data/collector.py`

Инструмент поддерживает:

* переключение пользователей
* назначение меток жестов с клавиатуры
* сохранение данных в CSV-файлы

Собранные данные хранятся локально и не добавляются в систему контроля версий.

---

## Обучение модели

Для обучения модели на собранных данных:

`PYTHONPATH=src python src/ml/train.py --dataset combined`

Скрипт обучения:

* загружает и проверяет датасет
* извлекает признаки
* обучает классификатор
* сохраняет модель и метаданные в единый файл

---

## Оффлайн-оценка

Для оценки модели на сохранённых данных:

`PYTHONPATH=src python src/ml/predict_one.py --n 500`

Для анализа одного примера:

`PYTHONPATH=src python src/ml/predict_one.py --row 0`

---

## Назначение проекта

Проект предназначен для:

* учебных и исследовательских целей
* курсовых и выпускных квалификационных работ
* исследования задач инклюзивного HCI и прикладного машинного обучения

Проект не является медицинским изделием и не предназначен для клинического применения.

---

## Возможные направления развития

* расширение словаря жестов
* улучшение обобщающей способности между пользователями
* использование временных моделей последовательностей
* интеграция с внешними приложениями или системами управления