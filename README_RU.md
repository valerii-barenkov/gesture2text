# Gesture2Text

![Python](https://img.shields.io/badge/Python-3.9%2B-blue)
![Platform](https://img.shields.io/badge/Platform-macOS%20%7C%20Linux%20%7C%20Windows-lightgrey)
![License](https://img.shields.io/badge/License-MIT-green)

**Gesture2Text** — исследовательский прототип, изучающий доступное камерное взаимодействие на основе классического машинного обучения.

**Gesture2Text** — это экспериментальная система распознавания жестов руки на основе машинного обучения, ориентированная на инклюзивное человеко-компьютерное взаимодействие и ассистивные технологии. Проект исследует, как модели машинного обучения на основе признаков могут обеспечивать базовую коммуникацию через заранее определённые жесты, распознаваемые стандартной веб-камерой без необходимости использования речи, сенсорного ввода или специализированного оборудования.

Система предназначена для людей с речевыми или моторными нарушениями, а также для сред, где голосовое управление ненадёжно (шум, требования к приватности). Вся обработка выполняется локально и офлайн.

---

## Демо
> Короткое демо (распознавание в реальном времени с веб-камеры) будет добавлено в одном из следующих обновлений.

---

## Основные возможности

* Классификация жестов на основе классического машинного обучения
* Распознавание жестов руки в реальном времени через веб-камеру
* Офлайн-инференс (без облачных сервисов)
* Лёгкий ML-пайплайн (классическое ML без deep learning на этапе инференса)
* Основано на **MediaPipe Hands** и **scikit-learn**
* Стабилизация жестов и фильтрация по уверенности
* Опциональный вывод текста в речь
* Модульная структура, подходящая для академических и исследовательских задач  

---

## Поддерживаемые жесты

Текущая версия поддерживает фиксированный словарь намеренных жестов:

* HELP
* STOP
* WATER
* PAIN
* YES
* NO
* CALL
* OK  

Также в системе присутствует специальный класс:

* **UNKNOWN** — используется для объединения всех жестов вне обученного набора и снижения количества ложных срабатываний  

Класс `UNKNOWN` намеренно включён для повышения безопасности и устойчивости системы. Он позволяет игнорировать неизвестные или неоднозначные положения руки. Набор жестов может быть расширен в будущих версиях путём сбора дополнительных данных и переобучения модели.

---

## Технический обзор

Модель обучается на собственноручно собранном датасете и оценивается на отложенной выборке для проверки обобщающей способности между жестами и пользователями.

Пайплайн обработки состоит из следующих этапов:

### 1. Детекция и отслеживание руки
MediaPipe Hands обнаруживает одну руку и выдаёт 21 трёхмерную ключевую точку на каждом кадре.

### 2. Извлечение признаков
Каждый кадр преобразуется в 63-мерный вектор признаков:

* Координаты центрируются относительно запястья
* Масштаб руки нормализуется по длине среднего пальца
* Левая и правая руки приводятся к единой системе координат
* Признаки разворачиваются в вектор фиксированной длины  

### 3. Классификация
Используется пайплайн scikit-learn:

* `StandardScaler` для нормализации
* Мультиномиальная логистическая регрессия для классификации  

### 4. Постобработка

* Временное сглаживание
* Порог уверенности
* Фильтрация через класс UNKNOWN для снижения ложных срабатываний

---

## Структура проекта

```
gesture2text/
├── src/
│   ├── app/
│   │   ├── main.py              # Entry point (optional wrapper)
│   │   └── run_camera.py        # Real-time webcam application
│   ├── ml/
│   │   ├── features.py          # Feature extraction logic
│   │   ├── train.py             # Model training script
│   │   └── predict_one.py       # Offline prediction / evaluation tool
│   ├── vision/
│   │   └── hand_tracker.py      # MediaPipe Hands wrapper
│   └── data/
│       ├── analyze_dataset.py   # Dataset inspection and sanity checks
│       ├── collector.py         # Interactive data collection tool
│       ├── raw/                 # Raw datasets (ignored by Git)
│       └── models/              # Trained models (ignored by Git)
├── requirements.txt
├── .gitignore
├── run.sh
├── run.command
└── run.bat
```

---

---

## Установка

### Клонирование репозитория

`git clone https://github.com/valerii-barenkov/gesture2text.git`  
`cd gesture2text`

### Создание и активация виртуального окружения

`python -m venv .venv`

**macOS / Linux:**  
`source .venv/bin/activate`

**Windows (PowerShell):**  
`.venv\Scripts\activate`

### Установка зависимостей

`pip install -r requirements.txt`

---

## Запуск приложения

**macOS / Linux:**  
`./run.sh`  
**или**  
`./run.command`

**Windows:**  
`run.bat`

**Либо напрямую:**  
`PYTHONPATH=src python src/app/run_camera.py`

Приложение запускает поток с веб-камеры и отображает распознанные жесты в реальном времени.

---

## Сбор данных

**Для сбора новых примеров жестов:**  
`PYTHONPATH=src python src/data/collector.py`

Этот инструмент позволяет:

* Переключать пользователей
* Назначать метки жестов с клавиатуры
* Сохранять собранные примеры в CSV-файлы

Собранные данные хранятся локально и не отслеживаются Git.

---

## Обучение модели

**Для обучения новой модели на собранных данных:**  
`PYTHONPATH=src python src/ml/train.py --dataset combined`

Скрипт обучения:

* Загружает и проверяет датасет
* Извлекает признаки
* Обучает классификатор
* Сохраняет итоговую модель (пайплайн и метаданные)

---

## Офлайн-оценка

**Для оценки модели на сохранённых примерах:**  
`PYTHONPATH=src python src/ml/predict_one.py --n 500`

**Для просмотра одного примера:**  
`PYTHONPATH=src python src/ml/predict_one.py --row 0`

---

## Результаты

* Обучение: собственный датасет жестов от нескольких пользователей
* Модель: базовая классическая ML-модель с классом UNKNOWN
* Статус: рабочий исследовательский прототип
* Оценка: метрики будут добавлены в будущих экспериментах

---

## Лицензия

Проект распространяется под лицензией MIT.
